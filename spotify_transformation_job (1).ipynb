{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1cfbe57f-4743-431b-b0aa-071956983124.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1cfbe57f-4743-431b-b0aa-071956983124.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 5.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1cfbe57f-4743-431b-b0aa-071956983124.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: None\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1cfbe57f-4743-431b-b0aa-071956983124.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: None\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import explode,col,to_date\nfrom datetime import datetime\nfrom awsglue.dynamicframe import DynamicFrame",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3_path = \"s3://spotify-etl-project-shubham/raw_data/to_processed/\"\nsource_dyf = glueContext.create_dynamic_frame_from_options(\n    connection_type=\"s3\",\n    connection_options={\"paths\":[s3_path]},\n    format=\"json\"\n)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spotify_df = source_dyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def process_albums(df):\n    df = df.withColumn(\"items\", explode(\"items\")).select(\n        col(\"items.track.album.id\").alias(\"album_id\"),\n        col(\"items.track.album.name\").alias(\"album_name\"),\n        col(\"items.track.album.release_date\").alias(\"release_date\"),\n        col(\"items.track.album.total_tracks\").alias(\"total_tracks\"),\n        col(\"items.track.album.external_urls.spotify\").alias(\"url\")\n    ).drop_duplicates([\"album_id\"])\n    return df\n\n\ndef process_artists(df):\n    # First, explode the items to get individual tracks\n    df_items_exploded = df.select(explode(col(\"items\")).alias(\"item\"))\n    \n    # Then, explode the artists array within each item to create a row for each artist\n    df_artists_exploded = df_items_exploded.select(explode(col(\"item.track.artists\")).alias(\"artist\"))\n    \n    # Now, select the artist attributes, ensuring each artist is in its own row\n    df_artists = df_artists_exploded.select(\n        col(\"artist.id\").alias(\"artist_id\"),\n        col(\"artist.name\").alias(\"artist_name\"),\n        col(\"artist.external_urls.spotify\").alias(\"external_url\")\n    ).drop_duplicates([\"artist_id\"])\n    \n    return df_artists\n\n\ndef process_songs(df):\n    # Explode the items array to create a row for each song\n    df_exploded = df.select(explode(col(\"items\")).alias(\"item\"))\n    \n    # Extract song information from the exploded DataFrame\n    df_songs = df_exploded.select(\n        col(\"item.track.id\").alias(\"song_id\"),\n        col(\"item.track.name\").alias(\"song_name\"),\n        col(\"item.track.duration_ms\").alias(\"duration_ms\"),\n        col(\"item.track.external_urls.spotify\").alias(\"url\"),\n        col(\"item.track.popularity\").alias(\"popularity\"),\n        col(\"item.added_at\").alias(\"song_added\"),\n        col(\"item.track.album.id\").alias(\"album_id\"),\n        col(\"item.track.artists\")[0][\"id\"].alias(\"artist_id\")\n    ).drop_duplicates([\"song_id\"])\n    \n    # Convert string dates in 'song_added' to actual date types\n    df_songs = df_songs.withColumn(\"song_added\", to_date(col(\"song_added\")))\n    \n    return df_songs\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 1cfbe57f-4743-431b-b0aa-071956983124\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\nWaiting for session 1cfbe57f-4743-431b-b0aa-071956983124 to get into ready status...\nSession 1cfbe57f-4743-431b-b0aa-071956983124 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "albums_df = process_albums(spotify_df)\nalbums_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+------------+------------+--------------------+\n|            album_id|          album_name|release_date|total_tracks|                 url|\n+--------------------+--------------------+------------+------------+--------------------+\n|07V9HO6Djetw5j5lX...|    So Close To What|  2025-02-20|          16|https://open.spot...|\n|07w0rG5TETcyihsEI...|                 SOS|  2022-12-09|          23|https://open.spot...|\n|0DLvFVIfwt3OHdK9k...|Where I've Been, ...|  2024-05-31|          12|https://open.spot...|\n|0TxewlKVKdpP18dGg...|       Love Somebody|  2024-10-18|           1|https://open.spot...|\n|0fSfkmx0tdPqFYkJu...|               MUSIC|  2025-03-14|          30|https://open.spot...|\n+--------------------+--------------------+------------+------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "artist_df = process_artists(spotify_df)\nartist_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-------------+--------------------+\n|           artist_id|  artist_name|        external_url|\n+--------------------+-------------+--------------------+\n|0Y5tJX1MQlPlqiwlO...| Travis Scott|https://open.spot...|\n|0du5cEVh5yTK9QJze...|   Bruno Mars|https://open.spot...|\n|0fTSzq9jAh4c36UVb...|  Alex Warren|https://open.spot...|\n|0iEtIxbK0KxaSlF7G...| Metro Boomin|https://open.spot...|\n|0ys2OFYzWYB5hRDLC...|Fuerza Regida|https://open.spot...|\n+--------------------+-------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "song_df = process_songs(spotify_df)\nsong_df.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+----------------+-----------+--------------------+----------+----------+--------------------+--------------------+\n|             song_id|       song_name|duration_ms|                 url|popularity|song_added|            album_id|           artist_id|\n+--------------------+----------------+-----------+--------------------+----------+----------+--------------------+--------------------+\n|00iLTetTLAeImmBlh...|           K POP|     112992|https://open.spot...|        73|2025-03-22|0fSfkmx0tdPqFYkJu...|699OTQXzgjhIYAHMy...|\n|0NUqi0ps17YpLUC3k...|      DIE TRYING|     195431|https://open.spot...|        87|2025-03-22|6Rl6YoCarF2GHPSQm...|2HPaUgqeutzr3jx5a...|\n|0UtnpKaReKUg2Gqua...|     Money Trees|     386906|https://open.spot...|        70|2025-03-22|748dZDqSZy6aPXKcI...|2YZyLoL8N0Wb9xBt1...|\n|0WbMK4wrZ1wFSty9F...|Good Luck, Babe!|     218423|https://open.spot...|        94|2025-03-22|1WAjjRMfZjEXtB0lQ...|7GlBOeep6PqTfFi59...|\n|0WiyWiJDkNCyGNqkv...|     Chasin' You|     205453|https://open.spot...|        84|2025-03-22|6WKNoni6aDzCTUN1C...|4oUHIQIBe0LHzYfvX...|\n+--------------------+----------------+-----------+--------------------+----------+----------+--------------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def write_to_s3(df, path_suffix, format_type=\"csv\"):\n    dynamic_frame = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\")\n    \n    glueContext.write_dynamic_frame.from_options(\n        frame=dynamic_frame,\n        connection_type=\"s3\",\n        connection_options={\"path\": f\"s3://spotify-etl-project-shubham/transformed_data/{path_suffix}/\"},\n        format=format_type\n    \n    )",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "write_to_s3(albums_df, \"album/album_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")),'csv')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "write_to_s3(artist_df, \"artist/artist_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")), \"csv\")\nwrite_to_s3(song_df, \"songs/songs_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")), \"csv\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def list_s3_objects(bucket,prefix):\n    s3_client = boto3.client(\"s3\")\n    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    keys = [content[\"Key\"] for content in response.get('Contents',[]) if content[\"Key\"].endswith(\".json\")]\n    return keys\n    \nbucket_name = \"spotify-etl-project-shubham\"\nprefix = \"raw_data/to_processed/\"\nspotify_keys = list_s3_objects(bucket_name,prefix)\n\ndef move_and_delete_files(spotify_keys,Bucket):\n    s3_resource = boto3.resource(\"s3\")\n    for key in spotify_keys:\n        copy_source = {\n            \"Bucket\" : Bucket,\n            \"Key\": key\n        }\n        \n        # Define the destination key\n        destination_key = \"raw_data/processed/\" + key.split(\"/\")[-1]\n        \n        # Copy the file to the new location\n        s3_resource.meta.client.copy(copy_source, Bucket, destination_key)\n        \n        # Delete the original file\n        s3_resource.Object(Bucket, key).delete()\n        \nmove_and_delete_files(spotify_keys,bucket_name)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}